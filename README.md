## start LLM
```
manthanchauhan@Manthans-MacBook-Pro llama.cpp % ./server/bin/llama-server -m ./models/Phi-3-mini-4k-instruct-q4.gguf --host 127.0.0.1 --port 8080
```
